{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# E²-CRF Caching Ablation Study and Results\n",
        "\n",
        "This notebook performs ablation studies on E²-CRF caching and tests the new method's performance.\n",
        "\n",
        "## Note: Generating results.yaml\n",
        "\n",
        "The `results.yaml` file in each `lightning_logs/{model_id}/` folder is generated by running the sampling script:\n",
        "\n",
        "```bash\n",
        "python cmd/sample.py model_id=XYZ num_samples=10000 num_diffusion_steps=1000\n",
        "```\n",
        "\n",
        "This script:\n",
        "1. Loads the trained model from checkpoint\n",
        "2. Generates samples using the diffusion process\n",
        "3. Computes metrics (Wasserstein distances, etc.) comparing generated samples with training data\n",
        "4. Saves results to `lightning_logs/{model_id}/results.yaml`\n",
        "5. Saves samples to `lightning_logs/{model_id}/samples.pt`\n",
        "\n",
        "The `results.yaml` contains:\n",
        "- `time_sliced_wasserstein_*`: Sliced Wasserstein distances in time domain\n",
        "- `freq_sliced_wasserstein_*`: Sliced Wasserstein distances in frequency domain\n",
        "- `time_marginal_wasserstein_*`: Marginal Wasserstein distances in time domain\n",
        "- `freq_marginal_wasserstein_*`: Marginal Wasserstein distances in frequency domain\n",
        "- `spectral_marginal_wasserstein_*`: Spectral density Wasserstein distances\n",
        "- Baseline metrics (self, dummy) for comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Optional, Dict, List\n",
        "import warnings\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scienceplots\n",
        "from hydra import compose, initialize\n",
        "from hydra.utils import instantiate\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "from fdiff.models.score_models import ScoreModule\n",
        "from fdiff.sampling.sampler import DiffusionSampler\n",
        "from fdiff.utils.extraction import get_best_checkpoint\n",
        "\n",
        "plt.style.use(\"science\")\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "runs_dir = Path.cwd() / \"../lightning_logs\"\n",
        "save_dir = Path.cwd() / \"../outputs\"\n",
        "\n",
        "# Create output directories\n",
        "(save_dir / \"figures\").mkdir(parents=True, exist_ok=True)\n",
        "(save_dir / \"tables\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Model ID - change this to your trained model\n",
        "model_id = \"latest\"  # or specify a specific model ID like \"03wb0ssr\"\n",
        "\n",
        "# Benchmark parameters\n",
        "num_samples = 20\n",
        "num_diffusion_steps = 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model(model_id: str) -> ScoreModule:\n",
        "    \"\"\"Load a trained model from checkpoint.\"\"\"\n",
        "    log_dir = Path(\"../lightning_logs\")\n",
        "    \n",
        "    if model_id == \"latest\":\n",
        "        checkpoints = list(log_dir.glob(\"*/checkpoints/*.ckpt\"))\n",
        "        if not checkpoints:\n",
        "            raise ValueError(\"No checkpoints found\")\n",
        "        checkpoint_path = max(checkpoints, key=lambda p: p.stat().st_mtime)\n",
        "        model_id = checkpoint_path.parent.parent.name\n",
        "        print(f\"Using latest model: {model_id}\")\n",
        "    else:\n",
        "        checkpoint_files = list((log_dir / model_id / \"checkpoints\").glob(\"*.ckpt\"))\n",
        "        if not checkpoint_files:\n",
        "            raise ValueError(f\"No checkpoint found for model_id: {model_id}\")\n",
        "        checkpoint_path = checkpoint_files[0]\n",
        "    \n",
        "    # Load model\n",
        "    score_model = ScoreModule.load_from_checkpoint(\n",
        "        str(checkpoint_path),\n",
        "        weights_only=False\n",
        "    )\n",
        "    score_model.eval()\n",
        "    \n",
        "    # Move to device\n",
        "    if torch.cuda.is_available():\n",
        "        score_model = score_model.cuda()\n",
        "        device = \"cuda\"\n",
        "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "        score_model = score_model.to('mps')\n",
        "        device = \"mps\"\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "    \n",
        "    print(f\"Model loaded on device: {device}\")\n",
        "    return score_model, model_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def benchmark_sampling(\n",
        "    score_model: ScoreModule,\n",
        "    num_samples: int = 10,\n",
        "    num_diffusion_steps: int = 100,\n",
        "    use_cache: bool = False,\n",
        "    cache_kwargs: Optional[dict] = None,\n",
        "    config_name: str = \"baseline\",\n",
        ") -> Dict:\n",
        "    \"\"\"Benchmark sampling with or without caching.\n",
        "    \n",
        "    Args:\n",
        "        score_model: The score model to use\n",
        "        num_samples: Number of samples to generate\n",
        "        num_diffusion_steps: Number of diffusion steps\n",
        "        use_cache: Whether to use caching\n",
        "        cache_kwargs: Optional cache configuration\n",
        "        config_name: Name of the configuration\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with timing and statistics\n",
        "    \"\"\"\n",
        "    sampler = DiffusionSampler(\n",
        "        score_model=score_model,\n",
        "        sample_batch_size=1,\n",
        "        use_cache=use_cache,\n",
        "        cache_kwargs=cache_kwargs or {},\n",
        "    )\n",
        "    \n",
        "    # Reset cache before benchmarking\n",
        "    if use_cache and score_model.cache is not None:\n",
        "        score_model.cache.reset()\n",
        "    \n",
        "    # Warmup\n",
        "    _ = sampler.sample(num_samples=1, num_diffusion_steps=10)\n",
        "    \n",
        "    # Reset cache again after warmup\n",
        "    if use_cache and score_model.cache is not None:\n",
        "        score_model.cache.reset()\n",
        "    \n",
        "    # Benchmark\n",
        "    start_time = time.time()\n",
        "    samples = sampler.sample(\n",
        "        num_samples=num_samples,\n",
        "        num_diffusion_steps=num_diffusion_steps\n",
        "    )\n",
        "    end_time = time.time()\n",
        "    \n",
        "    elapsed_time = end_time - start_time\n",
        "    \n",
        "    # Get cache statistics if available\n",
        "    cache_stats = {}\n",
        "    if use_cache and score_model.cache is not None:\n",
        "        cache_stats = score_model.cache.get_cache_stats()\n",
        "    \n",
        "    return {\n",
        "        \"config_name\": config_name,\n",
        "        \"elapsed_time\": elapsed_time,\n",
        "        \"samples\": samples,\n",
        "        \"cache_stats\": cache_stats,\n",
        "        \"num_samples\": num_samples,\n",
        "        \"num_diffusion_steps\": num_diffusion_steps,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using latest model: slxribxk\n",
            "Model loaded on device: mps\n",
            "Loaded model: slxribxk\n"
          ]
        }
      ],
      "source": [
        "score_model, actual_model_id = load_model(model_id)\n",
        "print(f\"Loaded model: {actual_model_id}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ablation Study: Different Caching Configurations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "E²-CRF Caching Ablation Study\n",
            "================================================================================\n",
            "\n",
            "1. Baseline (no caching)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                            4batch/s]\r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Time: 17.70s\n",
            "\n",
            "2. E²-CRF (full method, default settings)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                            8batch/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Time: 15.78s\n",
            "   Speedup: 1.12x\n",
            "   Cache Hit Ratio: 99.9%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "results = []\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"E²-CRF Caching Ablation Study\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 1. Baseline: No caching\n",
        "print(\"\\n1. Baseline (no caching)...\")\n",
        "result = benchmark_sampling(\n",
        "    score_model=score_model,\n",
        "    num_samples=num_samples,\n",
        "    num_diffusion_steps=num_diffusion_steps,\n",
        "    use_cache=False,\n",
        "    config_name=\"Baseline (No Cache)\",\n",
        ")\n",
        "results.append(result)\n",
        "baseline_time = result[\"elapsed_time\"]\n",
        "print(f\"   Time: {baseline_time:.2f}s\")\n",
        "\n",
        "# 2. E2-CRF: Full method (default settings)\n",
        "print(\"\\n2. E²-CRF (full method, default settings)...\")\n",
        "result = benchmark_sampling(\n",
        "    score_model=score_model,\n",
        "    num_samples=num_samples,\n",
        "    num_diffusion_steps=num_diffusion_steps,\n",
        "    use_cache=True,\n",
        "    cache_kwargs={},  # Default: K=5, R=10\n",
        "    config_name=\"E²-CRF (Default)\",\n",
        ")\n",
        "results.append(result)\n",
        "speedup = baseline_time / result[\"elapsed_time\"]\n",
        "cache_hit = result[\"cache_stats\"].get(\"cache_hit_ratio\", 0.0)\n",
        "print(f\"   Time: {result['elapsed_time']:.2f}s\")\n",
        "print(f\"   Speedup: {speedup:.2f}x\")\n",
        "print(f\"   Cache Hit Ratio: {cache_hit:.1%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "3. Ablation: Varying K (low-frequency tokens)...\n",
            "   K=0: "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                            8batch/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time: 15.69s, Speedup: 1.13x, Hit: 0.0%\n",
            "   K=3: "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Sampling:  40%|\u001b[34m████      \u001b[0m| 8/20 [00:06<00:09,  1.26batch/s]"
          ]
        }
      ],
      "source": [
        "# 3. Ablation: Varying K (low-frequency tokens)\n",
        "print(\"\\n3. Ablation: Varying K (low-frequency tokens)...\")\n",
        "for K in [0, 3, 5, 10, 15]:\n",
        "    print(f\"   K={K}: \", end=\"\", flush=True)\n",
        "    result = benchmark_sampling(\n",
        "        score_model=score_model,\n",
        "        num_samples=num_samples,\n",
        "        num_diffusion_steps=num_diffusion_steps,\n",
        "        use_cache=True,\n",
        "        cache_kwargs={\"K\": K},\n",
        "        config_name=f\"E²-CRF (K={K})\",\n",
        "    )\n",
        "    results.append(result)\n",
        "    speedup = baseline_time / result[\"elapsed_time\"]\n",
        "    cache_hit = result[\"cache_stats\"].get(\"cache_hit_ratio\", 0.0)\n",
        "    print(f\"Time: {result['elapsed_time']:.2f}s, Speedup: {speedup:.2f}x, Hit: {cache_hit:.1%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "4. Ablation: Varying R (refresh interval)...\n",
            "   R=5: "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                            9batch/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time: 15.60s, Speedup: 1.14x, Hit: 0.0%\n",
            "   R=10: "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                            9batch/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time: 15.62s, Speedup: 1.14x, Hit: 0.0%\n",
            "   R=20: "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                            8batch/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time: 15.63s, Speedup: 1.14x, Hit: 0.0%\n",
            "   R=50: "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                            8batch/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time: 15.79s, Speedup: 1.13x, Hit: 0.0%\n",
            "   R=100: "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                            5batch/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time: 16.05s, Speedup: 1.11x, Hit: 0.0%\n",
            "   R=500: "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                            9batch/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time: 15.50s, Speedup: 1.15x, Hit: 0.0%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "# 4. Ablation: Varying R (refresh interval)\n",
        "print(\"\\n4. Ablation: Varying R (refresh interval)...\")\n",
        "for R in [5, 10, 20, 50, 100, 500]:\n",
        "    print(f\"   R={R}: \", end=\"\", flush=True)\n",
        "    result = benchmark_sampling(\n",
        "        score_model=score_model,\n",
        "        num_samples=num_samples,\n",
        "        num_diffusion_steps=num_diffusion_steps,\n",
        "        use_cache=True,\n",
        "        cache_kwargs={\"R\": R},\n",
        "        config_name=f\"E²-CRF (R={R})\",\n",
        "    )\n",
        "    results.append(result)\n",
        "    speedup = baseline_time / result[\"elapsed_time\"]\n",
        "    cache_hit = result[\"cache_stats\"].get(\"cache_hit_ratio\", 0.0)\n",
        "    print(f\"Time: {result['elapsed_time']:.2f}s, Speedup: {speedup:.2f}x, Hit: {cache_hit:.1%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Ablation: Pure cache mode (no recomputation except step 0)\n",
        "print(\"\\n5. Ablation: Pure cache mode (R=inf)...\")\n",
        "result = benchmark_sampling(\n",
        "    score_model=score_model,\n",
        "    num_samples=num_samples,\n",
        "    num_diffusion_steps=num_diffusion_steps,\n",
        "    use_cache=True,\n",
        "    cache_kwargs={\"R\": 999999},  # Effectively no refresh\n",
        "    config_name=\"E²-CRF (Pure Cache)\",\n",
        ")\n",
        "results.append(result)\n",
        "speedup = baseline_time / result[\"elapsed_time\"]\n",
        "cache_hit = result[\"cache_stats\"].get(\"cache_hit_ratio\", 0.0)\n",
        "print(f\"   Time: {result['elapsed_time']:.2f}s\")\n",
        "print(f\"   Speedup: {speedup:.2f}x\")\n",
        "print(f\"   Cache Hit Ratio: {cache_hit:.1%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary DataFrame\n",
        "summary_data = []\n",
        "for r in results:\n",
        "    speedup = baseline_time / r[\"elapsed_time\"]\n",
        "    cache_stats = r.get(\"cache_stats\", {})\n",
        "    summary_data.append({\n",
        "        \"Configuration\": r[\"config_name\"],\n",
        "        \"Time (s)\": r[\"elapsed_time\"],\n",
        "        \"Speedup\": speedup,\n",
        "        \"Time per Sample (s)\": r[\"elapsed_time\"] / r[\"num_samples\"],\n",
        "        \"Time per Step (s)\": r[\"elapsed_time\"] / (r[\"num_samples\"] * r[\"num_diffusion_steps\"]),\n",
        "        \"Cache Hit Ratio\": cache_stats.get(\"cache_hit_ratio\", 0.0),\n",
        "        \"Cache Ratio\": cache_stats.get(\"cache_ratio\", 0.0),\n",
        "        \"Recompute Count\": cache_stats.get(\"recompute_count\", 0),\n",
        "        \"Cache Hit Count\": cache_stats.get(\"cache_hit_count\", 0),\n",
        "    })\n",
        "\n",
        "df_summary = pd.DataFrame(summary_data)\n",
        "df_summary = df_summary.round(3)\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Ablation Study Results Summary\")\n",
        "print(\"=\" * 80)\n",
        "print(df_summary.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 1: Speedup comparison\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "df_plot = df_summary[df_summary[\"Configuration\"] != \"Baseline (No Cache)\"]\n",
        "df_plot = df_plot.sort_values(\"Speedup\", ascending=False)\n",
        "ax.barh(df_plot[\"Configuration\"], df_plot[\"Speedup\"])\n",
        "ax.axvline(x=1.0, color='r', linestyle='--', label='Baseline (1.0x)')\n",
        "ax.set_xlabel(\"Speedup (×)\", fontsize=12)\n",
        "ax.set_title(\"E²-CRF Caching Speedup Comparison\", fontsize=14)\n",
        "ax.legend()\n",
        "ax.grid(axis='x', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(save_dir / \"figures/ablation_speedup.pdf\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 2: Cache hit ratio vs Speedup\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "df_plot = df_summary[df_summary[\"Cache Hit Ratio\"] > 0]\n",
        "ax.scatter(df_plot[\"Cache Hit Ratio\"], df_plot[\"Speedup\"], s=100, alpha=0.6)\n",
        "for idx, row in df_plot.iterrows():\n",
        "    ax.annotate(row[\"Configuration\"], \n",
        "                (row[\"Cache Hit Ratio\"], row[\"Speedup\"]),\n",
        "                fontsize=8, alpha=0.7)\n",
        "ax.set_xlabel(\"Cache Hit Ratio\", fontsize=12)\n",
        "ax.set_ylabel(\"Speedup (×)\", fontsize=12)\n",
        "ax.set_title(\"Cache Hit Ratio vs Speedup\", fontsize=14)\n",
        "ax.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(save_dir / \"figures/ablation_cache_hit_vs_speedup.pdf\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 3: K parameter sensitivity\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "df_k = df_summary[df_summary[\"Configuration\"].str.contains(\"K=\")]\n",
        "df_k[\"K\"] = df_k[\"Configuration\"].str.extract(r'K=(\\d+)').astype(float)\n",
        "df_k = df_k.sort_values(\"K\")\n",
        "\n",
        "ax1.plot(df_k[\"K\"], df_k[\"Speedup\"], marker='o', linewidth=2, markersize=8)\n",
        "ax1.set_xlabel(\"K (Low-frequency tokens)\", fontsize=12)\n",
        "ax1.set_ylabel(\"Speedup (×)\", fontsize=12)\n",
        "ax1.set_title(\"Speedup vs K Parameter\", fontsize=14)\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "ax2.plot(df_k[\"K\"], df_k[\"Cache Hit Ratio\"], marker='s', linewidth=2, markersize=8, color='orange')\n",
        "ax2.set_xlabel(\"K (Low-frequency tokens)\", fontsize=12)\n",
        "ax2.set_ylabel(\"Cache Hit Ratio\", fontsize=12)\n",
        "ax2.set_title(\"Cache Hit Ratio vs K Parameter\", fontsize=14)\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(save_dir / \"figures/ablation_k_sensitivity.pdf\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot 4: R parameter sensitivity\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "df_r = df_summary[df_summary[\"Configuration\"].str.contains(\"R=\")]\n",
        "df_r[\"R\"] = df_r[\"Configuration\"].str.extract(r'R=(\\d+)').astype(float)\n",
        "df_r = df_r.sort_values(\"R\")\n",
        "\n",
        "ax1.plot(df_r[\"R\"], df_r[\"Speedup\"], marker='o', linewidth=2, markersize=8)\n",
        "ax1.set_xlabel(\"R (Refresh interval)\", fontsize=12)\n",
        "ax1.set_ylabel(\"Speedup (×)\", fontsize=12)\n",
        "ax1.set_title(\"Speedup vs R Parameter\", fontsize=14)\n",
        "ax1.set_xscale('log')\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "ax2.plot(df_r[\"R\"], df_r[\"Cache Hit Ratio\"], marker='s', linewidth=2, markersize=8, color='orange')\n",
        "ax2.set_xlabel(\"R (Refresh interval)\", fontsize=12)\n",
        "ax2.set_ylabel(\"Cache Hit Ratio\", fontsize=12)\n",
        "ax2.set_title(\"Cache Hit Ratio vs R Parameter\", fontsize=14)\n",
        "ax2.set_xscale('log')\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(save_dir / \"figures/ablation_r_sensitivity.pdf\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## New Method (E²-CRF) vs Baseline Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare E²-CRF (default) with baseline\n",
        "baseline_result = results[0]  # Baseline\n",
        "e2crf_result = results[1]  # E²-CRF default\n",
        "\n",
        "comparison = {\n",
        "    \"Metric\": [\n",
        "        \"Total Time (s)\",\n",
        "        \"Time per Sample (s)\",\n",
        "        \"Time per Step (ms)\",\n",
        "        \"Speedup\",\n",
        "        \"Cache Hit Ratio\",\n",
        "        \"Cache Ratio\",\n",
        "    ],\n",
        "    \"Baseline\": [\n",
        "        baseline_result[\"elapsed_time\"],\n",
        "        baseline_result[\"elapsed_time\"] / baseline_result[\"num_samples\"],\n",
        "        baseline_result[\"elapsed_time\"] / (baseline_result[\"num_samples\"] * baseline_result[\"num_diffusion_steps\"]) * 1000,\n",
        "        1.0,\n",
        "        0.0,\n",
        "        0.0,\n",
        "    ],\n",
        "    \"E²-CRF\": [\n",
        "        e2crf_result[\"elapsed_time\"],\n",
        "        e2crf_result[\"elapsed_time\"] / e2crf_result[\"num_samples\"],\n",
        "        e2crf_result[\"elapsed_time\"] / (e2crf_result[\"num_samples\"] * e2crf_result[\"num_diffusion_steps\"]) * 1000,\n",
        "        baseline_result[\"elapsed_time\"] / e2crf_result[\"elapsed_time\"],\n",
        "        e2crf_result[\"cache_stats\"].get(\"cache_hit_ratio\", 0.0),\n",
        "        e2crf_result[\"cache_stats\"].get(\"cache_ratio\", 0.0),\n",
        "    ],\n",
        "}\n",
        "\n",
        "df_comparison = pd.DataFrame(comparison)\n",
        "df_comparison[\"Improvement\"] = df_comparison.apply(\n",
        "    lambda row: f\"{((row['Baseline'] - row['E²-CRF']) / row['Baseline'] * 100):.1f}%\" if row['Metric'] != 'Speedup' else f\"{row['E²-CRF']:.2f}x\",\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"E²-CRF vs Baseline Comparison\")\n",
        "print(\"=\" * 80)\n",
        "print(df_comparison.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Time comparison\n",
        "ax = axes[0, 0]\n",
        "metrics_to_plot = [\"Total Time (s)\", \"Time per Sample (s)\", \"Time per Step (ms)\"]\n",
        "baseline_vals = [df_comparison[df_comparison[\"Metric\"] == m][\"Baseline\"].values[0] for m in metrics_to_plot]\n",
        "e2crf_vals = [df_comparison[df_comparison[\"Metric\"] == m][\"E²-CRF\"].values[0] for m in metrics_to_plot]\n",
        "x = np.arange(len(metrics_to_plot))\n",
        "width = 0.35\n",
        "ax.bar(x - width/2, baseline_vals, width, label='Baseline', alpha=0.8)\n",
        "ax.bar(x + width/2, e2crf_vals, width, label='E²-CRF', alpha=0.8)\n",
        "ax.set_ylabel('Time', fontsize=12)\n",
        "ax.set_title('Time Comparison', fontsize=14)\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(metrics_to_plot, rotation=45, ha='right')\n",
        "ax.legend()\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Speedup\n",
        "ax = axes[0, 1]\n",
        "speedup_val = df_comparison[df_comparison[\"Metric\"] == \"Speedup\"][\"E²-CRF\"].values[0]\n",
        "ax.bar([\"E²-CRF\"], [speedup_val], alpha=0.8, color='green')\n",
        "ax.axhline(y=1.0, color='r', linestyle='--', label='Baseline (1.0x)')\n",
        "ax.set_ylabel('Speedup (×)', fontsize=12)\n",
        "ax.set_title('Speedup Achieved', fontsize=14)\n",
        "ax.legend()\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Cache statistics\n",
        "ax = axes[1, 0]\n",
        "cache_metrics = [\"Cache Hit Ratio\", \"Cache Ratio\"]\n",
        "cache_vals = [df_comparison[df_comparison[\"Metric\"] == m][\"E²-CRF\"].values[0] for m in cache_metrics]\n",
        "ax.bar(cache_metrics, cache_vals, alpha=0.8, color='orange')\n",
        "ax.set_ylabel('Ratio', fontsize=12)\n",
        "ax.set_title('Cache Statistics', fontsize=14)\n",
        "ax.set_ylim([0, 1])\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Time reduction percentage\n",
        "ax = axes[1, 1]\n",
        "time_reduction = (baseline_result[\"elapsed_time\"] - e2crf_result[\"elapsed_time\"]) / baseline_result[\"elapsed_time\"] * 100\n",
        "ax.bar([\"Time Reduction\"], [time_reduction], alpha=0.8, color='blue')\n",
        "ax.set_ylabel('Reduction (%)', fontsize=12)\n",
        "ax.set_title('Time Reduction', fontsize=14)\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(save_dir / \"figures/e2crf_vs_baseline.pdf\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results\n",
        "df_summary.to_csv(save_dir / \"tables/ablation_results.csv\", index=False)\n",
        "print(f\"Results saved to {save_dir / 'tables/ablation_results.csv'}\")\n",
        "\n",
        "# Save comparison table\n",
        "df_comparison.to_csv(save_dir / \"tables/e2crf_vs_baseline.csv\", index=False)\n",
        "print(f\"Comparison saved to {save_dir / 'tables/e2crf_vs_baseline.csv'}\")\n",
        "\n",
        "# Save LaTeX tables\n",
        "latex_table = df_summary.to_latex(index=False, float_format=\"%.3f\")\n",
        "with open(save_dir / \"tables/ablation_results.tex\", \"w\") as f:\n",
        "    f.write(latex_table)\n",
        "print(f\"LaTeX tables saved\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate results.yaml for a Model (Optional)\n",
        "\n",
        "If you need to generate `results.yaml` for a model that doesn't have it yet, you can run the sampling script. Here's a helper function to do it programmatically:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_results_yaml(\n",
        "    model_id: str,\n",
        "    num_samples: int = 10000,\n",
        "    num_diffusion_steps: int = 1000,\n",
        "    use_cache: bool = False,\n",
        "    cache_kwargs: Optional[dict] = None,\n",
        "    random_seed: int = 42,\n",
        ") -> dict:\n",
        "    \"\"\"Generate results.yaml for a trained model by running sampling and computing metrics.\n",
        "    \n",
        "    This replicates what `cmd/sample.py` does, but can be called from notebook.\n",
        "    \n",
        "    Args:\n",
        "        model_id: Model ID (folder name in lightning_logs)\n",
        "        num_samples: Number of samples to generate\n",
        "        num_diffusion_steps: Number of diffusion steps\n",
        "        use_cache: Whether to use caching\n",
        "        cache_kwargs: Optional cache configuration\n",
        "        random_seed: Random seed for reproducibility\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary containing the computed metrics\n",
        "    \"\"\"\n",
        "    from hydra.utils import instantiate\n",
        "    from omegaconf import OmegaConf\n",
        "    from fdiff.dataloaders.datamodules import Datamodule\n",
        "    from fdiff.models.score_models import ScoreModule\n",
        "    from fdiff.sampling.metrics import MetricCollection, SlicedWasserstein, MarginalWasserstein\n",
        "    from fdiff.sampling.sampler import DiffusionSampler\n",
        "    from fdiff.utils.extraction import get_best_checkpoint, get_model_type\n",
        "    from fdiff.utils.fourier import idft\n",
        "    from functools import partial\n",
        "    import yaml\n",
        "    \n",
        "    log_dir = Path(\"../lightning_logs\")\n",
        "    save_dir = log_dir / model_id\n",
        "    \n",
        "    if not save_dir.exists():\n",
        "        raise ValueError(f\"Model directory not found: {save_dir}\")\n",
        "    \n",
        "    # Load training config\n",
        "    train_cfg = OmegaConf.load(save_dir / \"train_config.yaml\")\n",
        "    \n",
        "    # Instantiate datamodule\n",
        "    datamodule: Datamodule = instantiate(train_cfg.datamodule)\n",
        "    datamodule.prepare_data()\n",
        "    datamodule.setup(stage=\"fit\")  # Use stage=\"fit\" for training data\n",
        "    \n",
        "    # Load model\n",
        "    best_checkpoint_path = get_best_checkpoint(save_dir / \"checkpoints\")\n",
        "    # Convert to dict for get_model_type (it accepts DictConfig | dict)\n",
        "    train_cfg_dict = OmegaConf.to_container(train_cfg, resolve=True)\n",
        "    assert isinstance(train_cfg_dict, dict)\n",
        "    model_type = get_model_type(train_cfg_dict)\n",
        "    score_model = model_type.load_from_checkpoint(\n",
        "        checkpoint_path=best_checkpoint_path,\n",
        "        weights_only=False\n",
        "    )\n",
        "    score_model.eval()\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        score_model = score_model.cuda()\n",
        "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "        score_model = score_model.to('mps')\n",
        "    \n",
        "    # Instantiate sampler\n",
        "    sampler = DiffusionSampler(\n",
        "        score_model=score_model,\n",
        "        sample_batch_size=1,\n",
        "        use_cache=use_cache,\n",
        "        cache_kwargs=cache_kwargs or {},\n",
        "    )\n",
        "    \n",
        "    # Instantiate metrics (use default metrics if not in config)\n",
        "    metrics_list = [\n",
        "        partial(SlicedWasserstein, random_seed=random_seed, num_directions=1000, save_all_distances=True),\n",
        "        partial(MarginalWasserstein, random_seed=random_seed, save_all_distances=True),\n",
        "    ]\n",
        "    \n",
        "    metrics = MetricCollection(\n",
        "        metrics=metrics_list,\n",
        "        original_samples=datamodule.X_train,\n",
        "        include_baselines=True,\n",
        "        include_spectral_density=True,\n",
        "    )\n",
        "    \n",
        "    # Generate samples\n",
        "    print(f\"Generating {num_samples} samples with {num_diffusion_steps} steps...\")\n",
        "    X = sampler.sample(\n",
        "        num_samples=num_samples,\n",
        "        num_diffusion_steps=num_diffusion_steps\n",
        "    )\n",
        "    \n",
        "    # Map to original scale if standardized\n",
        "    if datamodule.standardize:\n",
        "        feature_mean, feature_std = datamodule.feature_mean_and_std\n",
        "        X = X * feature_std + feature_mean\n",
        "    \n",
        "    # Convert to time domain if frequency domain\n",
        "    if datamodule.fourier_transform:\n",
        "        X = idft(X)\n",
        "    \n",
        "    # Compute metrics\n",
        "    print(\"Computing metrics...\")\n",
        "    results = metrics(X)\n",
        "    \n",
        "    # Save results\n",
        "    print(f\"Saving results to {save_dir / 'results.yaml'}...\")\n",
        "    with open(save_dir / \"results.yaml\", \"w\") as f:\n",
        "        yaml.dump(data=results, stream=f)\n",
        "    torch.save(X, save_dir / \"samples.pt\")\n",
        "    \n",
        "    print(\"Done! results.yaml and samples.pt have been saved.\")\n",
        "    return results\n",
        "\n",
        "# Example usage (uncomment to run):\n",
        "# results = generate_results_yaml(\n",
        "#     model_id=\"03wb0ssr\",\n",
        "#     num_samples=10000,\n",
        "#     num_diffusion_steps=1000,\n",
        "#     use_cache=False,  # Set to True to test with caching\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training with Diffusion Method Comparison\n",
        "\n",
        "This section shows how to train a model for 1 epoch and compare different diffusion methods during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To train with diffusion method comparison, run:\n",
        "# python cmd/train_diffusion_comparison.py trainer.max_epochs=1\n",
        "\n",
        "# Or use the config file:\n",
        "# python cmd/train_diffusion_comparison.py\n",
        "\n",
        "# This will:\n",
        "# 1. Train for 1 epoch\n",
        "# 2. Compare different diffusion methods at the end of the epoch\n",
        "# 3. Save results to lightning_logs/{run_id}/diffusion_comparison_results.csv\n",
        "\n",
        "print(\"To train with comparison, run:\")\n",
        "print(\"  python cmd/train_diffusion_comparison.py\")\n",
        "print(\"\\nOr override epochs:\")\n",
        "print(\"  python cmd/train_diffusion_comparison.py trainer.max_epochs=1\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Visualize Training Comparison Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_training_comparison_results(model_id: str) -> pd.DataFrame:\n",
        "    \"\"\"Load diffusion comparison results from training.\n",
        "    \n",
        "    Args:\n",
        "        model_id: Model ID (folder name in lightning_logs)\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with comparison results\n",
        "    \"\"\"\n",
        "    log_dir = Path(\"../lightning_logs\")\n",
        "    results_file = log_dir / model_id / \"diffusion_comparison_results.csv\"\n",
        "    \n",
        "    if not results_file.exists():\n",
        "        print(f\"Results file not found: {results_file}\")\n",
        "        print(\"Please run training with comparison first:\")\n",
        "        print(\"  python cmd/train_diffusion_comparison.py\")\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    df = pd.read_csv(results_file)\n",
        "    return df\n",
        "\n",
        "# Example: Load results for a model\n",
        "# comparison_df = load_training_comparison_results(\"your_model_id\")\n",
        "# if not comparison_df.empty:\n",
        "#     print(comparison_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_training_comparison(df: pd.DataFrame, save_dir: Path) -> None:\n",
        "    \"\"\"Visualize training comparison results.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with comparison results\n",
        "        save_dir: Directory to save figures\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        return\n",
        "    \n",
        "    # Plot 1: Time comparison by method\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    methods = df['method'].unique()\n",
        "    times = [df[df['method'] == m]['time'].mean() for m in methods]\n",
        "    ax.bar(methods, times, alpha=0.8)\n",
        "    ax.set_ylabel('Time (seconds)', fontsize=12)\n",
        "    ax.set_title('Diffusion Method Time Comparison (Training)', fontsize=14)\n",
        "    ax.set_xticklabels(methods, rotation=45, ha='right')\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_dir / \"figures/training_comparison_time.pdf\")\n",
        "    plt.show()\n",
        "    \n",
        "    # Plot 2: Cache hit ratio for cached methods\n",
        "    cached_methods = df[df['cache_hit_ratio'] > 0]\n",
        "    if not cached_methods.empty:\n",
        "        fig, ax = plt.subplots(figsize=(10, 6))\n",
        "        methods = cached_methods['method'].unique()\n",
        "        hit_ratios = [cached_methods[cached_methods['method'] == m]['cache_hit_ratio'].mean() for m in methods]\n",
        "        ax.bar(methods, hit_ratios, alpha=0.8, color='orange')\n",
        "        ax.set_ylabel('Cache Hit Ratio', fontsize=12)\n",
        "        ax.set_title('Cache Hit Ratio by Method (Training)', fontsize=14)\n",
        "        ax.set_xticklabels(methods, rotation=45, ha='right')\n",
        "        ax.set_ylim([0, 1])\n",
        "        ax.grid(axis='y', alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_dir / \"figures/training_comparison_cache_hit.pdf\")\n",
        "        plt.show()\n",
        "    \n",
        "    # Plot 3: Speedup comparison\n",
        "    baseline_time = df[df['method'] == 'baseline']['time'].mean()\n",
        "    if baseline_time > 0:\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "        methods = df['method'].unique()\n",
        "        speedups = [baseline_time / df[df['method'] == m]['time'].mean() for m in methods]\n",
        "        ax.bar(methods, speedups, alpha=0.8, color='green')\n",
        "        ax.axhline(y=1.0, color='r', linestyle='--', label='Baseline (1.0x)')\n",
        "        ax.set_ylabel('Speedup (×)', fontsize=12)\n",
        "        ax.set_title('Speedup Comparison (Training)', fontsize=14)\n",
        "        ax.set_xticklabels(methods, rotation=45, ha='right')\n",
        "        ax.legend()\n",
        "        ax.grid(axis='y', alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_dir / \"figures/training_comparison_speedup.pdf\")\n",
        "        plt.show()\n",
        "\n",
        "# Example usage:\n",
        "# comparison_df = load_training_comparison_results(\"your_model_id\")\n",
        "# if not comparison_df.empty:\n",
        "#     visualize_training_comparison(comparison_df, save_dir)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
